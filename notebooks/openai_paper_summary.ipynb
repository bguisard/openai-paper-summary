{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "E85dfHsJq0ol"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download files\n"
      ],
      "metadata": {
        "id": "E85dfHsJq0ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://arxiv.org/e-print/1906.05433 --output /content/1906.05433.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ztu_be0a8fJ",
        "outputId": "adc18764-249d-458a-a9c8-9481957a43e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 4366k  100 4366k    0     0  1085k      0  0:00:04  0:00:04 --:--:-- 1092k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf /content/1906.05433.tar.gz"
      ],
      "metadata": {
        "id": "YMojnFwpa8Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse Tex Files"
      ],
      "metadata": {
        "id": "q-jMIv-Hq-vE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vN2mhQrYXkaU"
      },
      "outputs": [],
      "source": [
        "!pip install texsoup==0.3.1 -qqq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "\n",
        "import TexSoup"
      ],
      "metadata": {
        "id": "A4ACNNoOYoDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_tex(tex_tree):\n",
        "    \"\"\"\n",
        "    Accepts a list of Union[TexNode,Token] and returns a nested list\n",
        "    of strings of the entire source document.\n",
        "\n",
        "    Adapted from:\n",
        "    https://github.com/alvinwan/TexSoup/blob/master/examples/list_everything.py\n",
        "    \"\"\"\n",
        "    text_chunks = []\n",
        "    for tex_code in tex_tree:\n",
        "        if isinstance(tex_code, TexSoup.data.TexNamedEnv):\n",
        "            text_chunks.extend(parse_tex(tex_code.all))\n",
        "        elif isinstance(tex_code, TexSoup.data.TexText):\n",
        "            if tex_code != \"\\n\":\n",
        "                text_chunks.append(tex_code)\n",
        "        elif isinstance(tex_code, TexSoup.data.TexGroup):\n",
        "            text_chunks.append([\"{\", parse_tex(TexSoup.TexSoup(tex_code.value).expr.all), \"}\"])\n",
        "        elif isinstance(tex_code, TexSoup.data.TexMathModeEnv):\n",
        "                text_chunks.append(tex_code.string)\n",
        "        elif isinstance(tex_code, TexSoup.data.TexCmd) or isinstance(tex_code, TexSoup.utils.Token):\n",
        "            # Skip parsing TexCmds and tokens\n",
        "            continue\n",
        "        else:\n",
        "            print(f\"unable to parse {tex_code} [type={type(tex_code)}]\")\n",
        "            continue\n",
        "\n",
        "    return text_chunks\n",
        "\n",
        "def parse_document(filepath: str) -> str:\n",
        "    with open(filepath) as f:\n",
        "        data = f.read()\n",
        "\n",
        "    soup = TexSoup.TexSoup(data)\n",
        "\n",
        "    text_chunks = parse_tex(soup.expr.all)\n",
        "\n",
        "    result = \"\".join(text_chunks)\n",
        "    \n",
        "    # Clean up some weird artifacts.\n",
        "    result = result.replace(\"\\n\", \" \")\n",
        "    result = result.replace(\"\\\\\", \"\")\n",
        "    result = result.replace(\"\\\\\\\\\", \"\")\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "HkyWj-Zbho53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sections = {}\n",
        "\n",
        "path = pathlib.Path(\"sections\")  \n",
        "for f in list(path.iterdir()):\n",
        "    if f.is_file() and f.name.endswith(\".tex\"):\n",
        "        section_name = f.name.split(\".\")[0]\n",
        "        if section_name in [\"acknowledgments\"]:\n",
        "            continue\n",
        "\n",
        "        print(f\"parsing {f.name}\")\n",
        "        sections[section_name] = parse_document(str(f.absolute()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiCHd1Ibuya3",
        "outputId": "4a779e0d-a584-4b6f-fc93-3f882270451a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parsing societalImpacts.tex\n",
            "parsing toolsIndividuals.tex\n",
            "parsing electricitySystems.tex\n",
            "parsing geoengineering.tex\n",
            "parsing intro.tex\n",
            "parsing climateModels.tex\n",
            "parsing transportation.tex\n",
            "parsing conclusion.tex\n",
            "parsing finance.tex\n",
            "parsing toolsSociety.tex\n",
            "parsing ccs.tex\n",
            "parsing buildingsCities.tex\n",
            "parsing agricultureForestryLand.tex\n",
            "parsing industry.tex\n",
            "parsing education.tex\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split text into smaller chunks"
      ],
      "metadata": {
        "id": "FB9HJHhDzXV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -qqq"
      ],
      "metadata": {
        "id": "KctHGEmc1IGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "import nltk\n",
        "import transformers\n",
        " \n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3rGR093zfRd",
        "outputId": "aff4f56d-cf2e-402d-8627-f8ef06fd0d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_chunks(\n",
        "    input_text: str, \n",
        "    tokenizer: transformers.PreTrainedTokenizer, \n",
        "    max_token_sz: int = 1024, \n",
        "    overlapping_sentences: int = 10,\n",
        ") -> List[str]:\n",
        "\n",
        "    sentences = nltk.sent_tokenize(input_text)\n",
        "\n",
        "    chunks = []\n",
        "\n",
        "    first_sentence = 0\n",
        "    last_sentence = 0\n",
        "    while last_sentence <= len(sentences) - 1:\n",
        "        last_sentence = first_sentence\n",
        "        chunk_parts = []\n",
        "        chunk_size = 0\n",
        "        for sentence in sentences[first_sentence:]:\n",
        "            sentence_sz = len(tokenizer.encode(sentence))\n",
        "            if chunk_size + sentence_sz > max_token_sz:\n",
        "                break\n",
        "            \n",
        "            chunk_parts.append(sentence)\n",
        "            chunk_size += sentence_sz\n",
        "            last_sentence += 1\n",
        "\n",
        "        chunks.append(\" \".join(chunk_parts))\n",
        "        first_sentence = last_sentence - overlapping_sentences\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "l5cSUjgv0YCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "section_chunks = {}\n",
        "for section, section_text in sections.items():\n",
        "    section_chunks[section] = text_to_chunks(section_text, tokenizer)"
      ],
      "metadata": {
        "id": "jK_Ct1h_2qSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for section, chunks in section_chunks.items():\n",
        "    print(f\"{section}: {len(chunks)} chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEKVQIfW3h2_",
        "outputId": "5a2918db-531d-426d-a44d-b4977265a65d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "societalImpacts: 4 chunks.\n",
            "toolsIndividuals: 2 chunks.\n",
            "electricitySystems: 6 chunks.\n",
            "geoengineering: 2 chunks.\n",
            "intro: 2 chunks.\n",
            "climateModels: 3 chunks.\n",
            "transportation: 6 chunks.\n",
            "conclusion: 1 chunks.\n",
            "finance: 1 chunks.\n",
            "toolsSociety: 4 chunks.\n",
            "ccs: 1 chunks.\n",
            "buildingsCities: 5 chunks.\n",
            "agricultureForestryLand: 3 chunks.\n",
            "industry: 3 chunks.\n",
            "education: 1 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarize chunks"
      ],
      "metadata": {
        "id": "bUmwNC8A4-rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai tenacity -qqq"
      ],
      "metadata": {
        "id": "RcbgOj3l4dPy"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "from google.colab import drive\n",
        "import openai\n",
        "\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        ")"
      ],
      "metadata": {
        "id": "JaH9f_co-B4v"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "VOF8sLG2FrPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/openai.key') as f:\n",
        "    openai.api_key = f.read().strip()"
      ],
      "metadata": {
        "id": "IDYSs8pzFtiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(5))\n",
        "def completion_with_backoff(**kwargs):\n",
        "    return openai.Completion.create(**kwargs)\n",
        "\n",
        "\n",
        "def summarize_chunk(chunk: str, max_tokens: int = 512, temperature: int = 0) -> str:\n",
        "    response = completion_with_backoff(\n",
        "        model=\"text-davinci-002\",\n",
        "        prompt=f'Using scientific language, provide a long and detailed summary of the excerpt below\".'\n",
        "        f\"\\n###\\nExcerpt:{chunk}\\n###\\n-\",\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "    )\n",
        "\n",
        "    return response['choices'][0]['text'].strip()"
      ],
      "metadata": {
        "id": "i9HG4DNoETSW"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = defaultdict(list)\n",
        "for section, chunks in section_chunks.items():\n",
        "    for chunk in chunks:\n",
        "        summaries[section].append(summarize_chunk(chunk))"
      ],
      "metadata": {
        "id": "oTXV3zd8-Kc7"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_path = pathlib.Path('/content/summaries')\n",
        "out_path.mkdir(parents=False, exist_ok=True)"
      ],
      "metadata": {
        "id": "o2eGuOMxGjLo"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for section, summs in summaries.items():\n",
        "    fp = out_path / f\"{section}.txt\"\n",
        "    with fp.open(mode='w') as f:\n",
        "        summary = \"\\n\".join(summs)\n",
        "        f.write(summary)"
      ],
      "metadata": {
        "id": "I4qYqUJlJOCF"
      },
      "execution_count": 104,
      "outputs": []
    }
  ]
}